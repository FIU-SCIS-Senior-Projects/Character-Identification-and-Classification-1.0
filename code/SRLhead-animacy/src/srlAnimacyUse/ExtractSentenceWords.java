package srlAnimacyUse;

import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.LinkedHashMap;
import java.util.Map;

import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.parsers.ParserConfigurationException;

import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;
import org.xml.sax.SAXException;

/**
 * Get the whole sentence and get the output to a file
 * @author Geeticka Chauhan
 *
 */
public class ExtractSentenceWords 
{
	static String basePath;
	public ExtractSentenceWords(String bp)
	{
		this.basePath = bp;
	}

	/**
	 * Extract the words in the sentence (stories 1-15) based on the start and end token number from input file
	 * @param args
	 * @throws ParserConfigurationException
	 * @throws SAXException
	 * @throws IOException
	 */
	public void Extract(String args[]) throws ParserConfigurationException, SAXException, IOException
	{
		//data used is that generated by the stanford NLP parser
		/*
		String files[] = { basePath + "/output/sentences/Story1.xml",
				basePath + "/output/sentences/Story2.xml",
				basePath + "/output/sentences/Story3.xml",
				basePath + "/output/sentences/Story4.xml",
				basePath + "/output/sentences/Story5.xml",
				basePath + "/output/sentences/Story6.xml",
				basePath + "/output/sentences/Story7.xml",
				basePath + "/output/sentences/Story8.xml",
				basePath + "/output/sentences/Story9.xml",
				basePath + "/output/sentences/Story10.xml",
				basePath + "/output/sentences/Story11.xml",
				basePath + "/output/sentences/Story12.xml",
				basePath + "/output/sentences/Story13.xml",
				basePath + "/output/sentences/Story14.xml",
				basePath + "/output/sentences/Story15.xml"};
		*/
	    
		for(int i=1; i<=15; i++)
		{ // for every file
			   
		// set up optional output files
	    PrintWriter out;
		if (args.length > 1) {
		out = new PrintWriter(args[1]);
		} else {
		out = new PrintWriter(System.out);
		  }
		PrintWriter xmlOut = new PrintWriter(new File(basePath + "/output/sentenceWords/story" + i + ".txt"));
		/*
		if (args.length > 2) {
		xmlOut = new PrintWriter(args[2]);
	    }
	    */
		
		
		 // here we start reading the original input file to extract the actual words that are needed
        File inputFile2 = new File(basePath + "/input/story" + i + ".xml");
		 DocumentBuilderFactory dbFactory2 = DocumentBuilderFactory.newInstance();
	     DocumentBuilder dBuilder2 = dbFactory2.newDocumentBuilder();
	     Document doc2 = dBuilder2.parse(inputFile2);
        doc2.getDocumentElement().normalize();
        
        NodeList parses = doc2.getElementsByTagName("rep");
        // extract the parse element that has all the tokens 
        Element neededParseElement = null;
        
        for (int temp2 = 0; temp2 < parses.getLength(); temp2++)  
        { // for every sentence element
    		Node parse = parses.item(temp2);
    		//System.out.println("\nCurrent Element :" + nNode.getNodeName());

    		if (parse.getNodeType() == Node.ELEMENT_NODE) 
    		{
    			// below is each sentence element
    			Element parseElement = (Element) parse;
    			// check for whichever rep has the id corresponding to sentences
    			if(parseElement.getAttribute("id").equals("edu.mit.parsing.token"))
    			{
    				neededParseElement = parseElement;
    				break;
    			}
    		}
        }
        
        NodeList tokens = neededParseElement.getElementsByTagName("desc");
        // all the tokens 
        
        
        // extract the sentences
		File inputFile = new File(basePath + "/output/sentences/story" + i + ".xml");
        DocumentBuilderFactory dbFactory = DocumentBuilderFactory.newInstance();
        DocumentBuilder dBuilder = dbFactory.newDocumentBuilder();
        Document doc = dBuilder.parse(inputFile);
        doc.getDocumentElement().normalize();
        System.out.println("File: " + i);
        System.out.println("Root element :" + doc.getDocumentElement().getNodeName());
        // get all the rep tags
        NodeList sentences = doc.getElementsByTagName("sentence");
      
        int startingToken = 0; // the number that keeps track of the token offset from the beginning 
        for (int temp = 0; temp < sentences.getLength(); temp++)  
        { // for every sentence element
    		Node sentence = sentences.item(temp);
    		//System.out.println("\nCurrent Element :" + nNode.getNodeName());
    		//xmlOut.print("<sentence id=\"" + (temp + 1) + "\">");
    		System.out.println("sentence " + temp + " of story " + i);
    		if (sentence.getNodeType() == Node.ELEMENT_NODE) 
    		{
    			// below is each sentence element
    			Element sentenceElement = (Element) sentence;
    			
    			int start = Integer.parseInt(sentenceElement.getElementsByTagName("start").item(0).getTextContent());
    			//if(temp == 0) startingToken = start;
    			int end = Integer.parseInt(sentenceElement.getElementsByTagName("end").item(0).getTextContent());
    			// simultaneously parse through the tokens according to what start and end says
    			// an idea is to keep deleting the elements from the tokens while reading through it
    			// somehow can keep a count of the start at which you actually are
    			// start is the token id of the starting element 
    			
    			int r = 0;
    			int neededIndex = 0; // this is the index where the tokens for the sentence start 
    			while(r < tokens.getLength())
    			{
    				Node token = tokens.item(r);
    				if(token.getNodeType() == Node.ELEMENT_NODE)
    				{
    					Element tokenElement = (Element) token;
    					if(Integer.parseInt(tokenElement.getAttribute("id")) == start)
    					{
    						neededIndex = r;
    						break;
    					}	
    					r++;
    				}
    			}
    			System.out.println("\tStarting index in tokens " + neededIndex);
    			
    			int f = neededIndex;
    			
    			while(f < tokens.getLength())
    			{
    				//System.out.println("\t\t\t\t token" + f);
    				Node token = tokens.item(f);
    				if(token.getNodeType() == Node.ELEMENT_NODE)
    				{
    					Element tokenElement = (Element) token;
    					System.out.println("\t\t\t\t token" + tokenElement.getAttribute("id"));
    					xmlOut.print(tokenElement.getTextContent() + " ");
    					if(Integer.parseInt(tokenElement.getAttribute("id")) == end)
    					{
    						break;
    					}
    					
    					
    					f++;
    				}
    					
    			}
    		/*
    			for(int f = start; f <= end; f++) 
    			{
    				// f-start ensures the index starts at 0, and adding startingToken ensures that the correct token offset is set
    				Node token = tokens.item((f - start) + startingToken); // to correct the indexing ;; some number needs to be added that keeps count for tokens between sentences
    				if(token.getNodeType() == Node.ELEMENT_NODE)
    	        	{
    					Element tokenElement = (Element) token;
    					xmlOut.print(tokenElement.getTextContent() + " ");
    					
    	        	}
    			}
    			
    			startingToken += (end - start) + 1; // get the
    			*/
    			xmlOut.println();
    		//	xmlOut.print("</sentence>");

    			
    		}
        }
        
      
        
        System.out.println("----------------------------");
        
        out.close();
        xmlOut.close();
   //     IOUtils.closeIgnoringExceptions(out);
	 //   IOUtils.closeIgnoringExceptions(xmlOut);
      }
	}
	
}
